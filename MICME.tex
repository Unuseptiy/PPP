%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Основной файл для авторов журнала MICME
%%%%% факультет математики и информатики
%%%%% Таврическая академия Крымского федерального университета им. В.И. Вернадского
%%%%% e-mail: micme2017@yandex.ru
%%%%% site: http://micme.cfuv.ru/ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%7 - 1, 
%9 - 2, 
%4 - 3,
%5 - 4,
%6 - 5
%ПРАВИЛА ОФОРМЛЕНИЯ МАТЕРИАЛОВ:
%
%Материалы должны быть оформлены с использованием стилевого файла micmo.cls по образцу примера MICME.tex в редакторе LaTeX на русском или английском языке.
%
%Название файла формируйте следующим образом Surname.tex
%Surname – фамилия, написанная по-английски без пробелов.



\documentclass{micmo}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english,ukrainian,russian]{babel}

%
\usepackage{moreverb}	
%

\sloppy
%\newcommand{\ds}{\displaystyle}

\topmatter{В.\;С.\;Козенко} %укажите свои инициалы и фамилию в предложенном формате

\udc{004.08} %укажите УДК вашей работы

\title{Сравнительный анализ библиотек глубокого обучения} %укажите название вашей работы

%\thanks{Работа выполнена при финансовой поддержке~\ldots}

\author{В.\;С.\;Козенко}%\author{инициалы и фамилия}
       {Крымский федеральный университет им.\;В.\;И.\;Вернадского, \textit{e-mail: mail@.ru}} %{контактные данные}
       
\date{01.03.2018} %дата подачи работы

\enabstracts{68T99}%\enabstracts{обозначения MSC2010}
  {V.\;S.\;Kozenko}%{инициалы и фамилия на английском языке}
  {V.\;I.\;Vernadsky Crimean Federal University} %{контактные данные на английском языке}
  {Comparative analisis of deep learning libraries}%{название работы на английском языке}
  {This article explains the comparison of the deep learning libraries TensorFlow, PyTorch, Keras. The comparison results are presented in the form of a table at the end of the article.}%{аннотация на английском языке}
  {Deep learning, neural network, libraries.}%{ключевые слова на английском языке} 
 
\begin{document}
\begin{article}

\abstracts{Данная статья поясвящена сравнению библиотек глубокого обучения TensorFlow, PyTorch, Keras. Итоги сравнения представлены в виде таблицы в конце статьи.}%{текст аннотации}
{Глубокое обучение, нейросети, библиотеки, TensorFlow, PyTorch, Keras.} %{ключевые слова}
\section*{Введение}
TensorFlow --- открытая библиотека для машинного обучения, разработанная компанией Google, для решения задач построения и тренировки нейронных сетей. Основной API для работы с библиотекой реализован для Python также существуют реализации для C Sharp, C++, Haskell, Java, Go и Swift. Является продолжением закрытого проекта DistBelief. Изначально TensorFlow была разработана командой Google Brain для внутреннего использования в Google, в 2015 году система была переведена в свободный доступ~\cite{wikiT}.

PyTorch --- библиотека машинного обучения для языка Python с открытым исходным кодом, созданная на базе Torch. Разрабатывается преимущественно группой искусственного интеллекта Facebook. Также вокруг этого фреймворка выстроена экосистема, состоящая из различных библиотек, разрабатываемых сторонними командами: Fast.ai, упрощающая процесс обучения моделей, Pyro, модуль для вероятностного программирования, от Uber, Flair, для обработки естественного языка и Catalyst, для обучения DL и RL моделей~\cite{wikiP}.

Keras --- открытая нейросетевая библиотека, написанная на языке Python. Она представляет собой надстройку над фреймворками Deeplearning4j, TensorFlow и Theano. Нацелена на оперативную работу с сетями глубинного обучения, при этом спроектирована так, чтобы быть компактной, модульной и расширяемой. Она была создана как часть исследовательских усилий проекта ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), а ее основным автором и поддерживающим является Франсуа Шолле (Fran?ois Chollet), инженер Google. Планировалось что Google будет поддерживать Keras в основной библиотеке TensorFlow, однако Шолле выделил Keras в отдельную надстройку, так как согласно концепции Keras является скорее интерфейсом, чем сквозной системой машинного обучения. Keras предоставляет высокоуровневый, более интуитивный набор абстракций, который делает простым формирование нейронных сетей, независимо от используемой в качестве вычислительного бэкенда библиотеки научных вычислений. Microsoft работает над добавлением к Keras и низкоуровневых библиотек CNTK~\cite{wikiK}.

\section*{Сравнение библиотек глубокого обучения}
\subsection*{Теоритическое сравнение}
Все современные фреймворки можно разделить на три крупных класса:
\begin{itemize}
	\item[$1)$] Фреймворки с фиксированные модулями.
	
	В таких фреймворках пользователь комбинирует уже готовые блоки в граф вычислений и запускает его. Прямой и обратный проход уже предусмотрен в каждом блоке. Расширяемость близка к нулю: определять новые блоки гораздо сложнее, чем использовать уже готовые. Однако, если готовых блоков достаточно для вашей задачи, то скорость разработки максимальна, как и скорость работы программы (т.к. заранее написанный код высоко оптимизирован). Такие библиотеки используются для прототипирования. К ним относится Keras, а так же Caffe, Caffe2, CNTK, Kaldi, DL4J.
	\item[$2)$] Фреймворки со статическим графом вычислений.
	
	С помощью таких фреймворков уже можно создавать вычислительные графы произвольной сложности и размера, однако, после компиляции, в нем ничего изменить нельзя будет: он будет доступен только для прямого и обратного проходов. Из-за этого возрастает сложность отладки программы. Все такие фреймворки используют декларативный стиль программирования и напоминают функциональный язык или математическую нотацию. Представителями этого класса являются TensorFlow, а так же Theano, MXNet.
	\item[$3)$] Фреймворки с динамическим графом вычислений.
	
	В фреймворках данного класса граф вычислений строится при каждом запуске программы, при каждом проходе по нему, в отличии от предыдущего класса. Подобный подход даёт максимальную гибкость и расширяемость, позволяет использовать в вычислениях все возможности используемого языка программирования и не ограничивает пользователя вообще ничем. К этому классу фреймворков относятся Torch, PyTorch, Chainer, DyNet.
\end{itemize}

Сравним ярких представителей этих классов.

Видим, что у PyTorch граф вычислений динамический, у TensorFlow --- статический, а у Keras --- зависит от того, как интерфейс какой библиотеки мы его используем.

Из ограничений, накладываемых видом графа вычислений вытекает следующее отличие: логичность построенной программы. Сравним два кода с использованием цикла \verb'while':
\begin{itemize}
	\item[$-$] на TensorFlow:
	\begin{listing}{1}
import tensorflow as tf
x = tf.constant(2, shape=[2, 2]) 
w = tf.while_loop(
	lambda x: tf.reduce_sum(x) < 100, 
	lambda x: tf.nn.relu(tf.square(x)), 
	[x])
	\end{listing}
	\item[$-$] на PyTorch:
	\begin{listing}{1}
import torch.nn
from torch.autograd import Variable
x = Variable(torch.ones([2, 2]) * 2) 
while x.sum() < 100:
	x = torch.nn.ReLU()(x**2)
	\end{listing}
\end{itemize}

Циклы в графах TensorFlow следует представлять как операции \verb'tf.while()', принимающей на вход \verb'condition' и подграф \verb'body'. Так же и с операцией ветвления: она принимает в качестве ввода три параметра: условный подграф и два подграфа для двух веток развития условия: \verb'if' и \verb'else'. А для реализации этих операций на PyTorch нам хватает просто языка Python, и эта реализация выглядит логичнее.

Каждый из фреймворков поддерживает реализацию новых операций. Для PyTorch их можно написать на Python и на С++~\cite{PyTorch}, для TensorFlow на С++~\cite{TensorFlow}, для Keras на Python~\cite{Keras}. Разработчики предоставляют инструкции по данным вопросам.

Специалисты придерживаются мнения, что у TensorFlow плохое качество документации, в отличии от Keras и PyTorch~\cite{sim0nSays}.

Связав все предыдущие параметры воедино, а так же заметив, что API TensorFlow предоставляет лишь простейшие инструкции по сборке, необходимые для создания расчетных графов, он лишен «стандартной библиотеки» для наиболее распространенных фрагментов программ. Поэтому более высокоуровневые API поверх TensorFlow реализованы сообществом, из-за чего их появилось множество и они совершенно различны (наиболее популярные из этих API: Keras, TFLearn, PrettyTensor, TF-Slim). В случае с PyTorch все сложилось иначе: он уже оснащен самыми ходовыми элементами, нужными для ежедневных исследований в области глубокого обучения. В PyTorch есть нативный Keras-подобный API в пакете torch.nn.

\begin{tabular}[t]{| p{60pt} | p{75pt} | p{67pt} | p{64pt} | p{60pt} | p{60pt} |}
	\multicolumn{6}{r}{Таблица 1. 1.}\\
	\hline
	& \bf{Тип графа} & \bf{Логичность} & \bf{Поддержка новых операций} & \bf{Качество документации} & \bf{Удобность API} \\
	\hline
	\bf{PyTorch} & Динамический & Высокая & Python, C++ &  Высокое & Выслокая\\
	\hline
	\bf{TensorFlow} & Статический & Средняя & С++ & Среднее & Средняя\\
	\hline
	\bf{Keras} & Зависит от backend & Высокая & Python & Высокое & Средняя\\
	\hline
\end{tabular}

\subsection*{Практическое сравнение/Программная реализация}

Код программы классификации набора изображений рукописных цифр MNIST с использованием библиотеки Keras:
\begin{listing}{1}
import tensorflow as  tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
mnist = keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images / 255.0 
test_images = test_images / 255.0 
model = keras.Sequential([
	keras.layers.Flatten(input_shape=(28, 28)), 
	keras.layers.Dense(128, activation=tf.nn.relu), 
	keras.layers.Dense(10, activation=tf.nn.softmax)
	]) 
model.compile(
	optimizer=tf.compat.v1.train.AdamOptimizer(),
	loss='sparse_categorical_crossentropy',
	metrics=[‘accuracy'])
model.fit(train_images, train_labels, epochs=5)
test_loss, test_acc = model.evaluate(test_images, test_labels) 
print('Test accuracy:', test_acc)
\end{listing}

Код программы классификации набора изображений рукописных цифр MNIST с использованием библиотеки PyTorch:
\begin{listing}{1}
import torch
import torchvision
from torchvision import transforms, datasets import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
train = datasets.MNIST(
	'', 
	train=True, download=True, 	
	transform=transforms.Compose([
	transforms.ToTensor() ]))
test = datasets.MNIST(
	'', 
	train=False, 
	download=True, 
	transform=transforms.Compose([
	transforms.ToTensor() ]))
trainset = torch.utils.data.DataLoader(
	train, 
	batch_size=10, 
	shuffle=True)
testset = torch.utils.data.DataLoader(
	test, 
	batch_size=10, 
	shuffle=False)
class Net(nn.Module): 
	def __init__(self):
		super().__init__()
		self.fc1 = nn.Linear(28*28, 128) 
		self.fc2 = nn.Linear(128, 10)
	def forward(self, x):
		x = F.relu(self.fc1(x))
		x = self.fc2(x)
		return F.log_softmax(x, dim=1)
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001) 
for epoch in range(5):
	for data in trainset: 
		X, y = data
		net.zero_grad()
		output = net(X.view(-1,784)) 
		loss = F.nll_loss(output, y) 
		loss.backward() 
		optimizer.step()
	print(loss)
correct = 0
total = 0
with torch.no_grad(): 
	for data in testset:
		X, y = data
		output = net(X.view(-1,784)) 
		#print(output)
		for idx, i in enumerate(output):
		#print(torch.argmax(i), y[idx]) 
		if torch.argmax(i) == y[idx]:
			correct += 1 
		total += 1
print("Accuracy: ", round(correct/total, 3))
\end{listing}

\section*{Заключение}

Несмотря на то, что понятия «глубокое обучение», «искусственный интеллект», «машинное обучение» только недавно стали очень популярны и широко обсуждаемы, в науке они начали разрабатываться около полувека назад. Основой этой области являются программирование и такие разделы математики: теория вероятности, методы оптимизации, математический анализ, линейная алгебра. Учеными за время существования глубокого обучения было разработано множество библиотек. Мы разделил их на три основных класса и сравнили самых ярких представителей из них. Выбирая между библиотеками, использующими статический и динамический граф стоит брать во внимание: динамизм может как оптимизировать программируемость, так и ухудшать производительность --- то есть, оптимизировать такие графы сложнее. Поэтому, отличия и компромиссы между PyTorch и TensorFlow во многом такие же, как и между динамическим интерпретируемым языком, например, Python, и статическим компилируемым языком, например, C или C++. Первый проще и работать с ним быстрее, а из второго и третьего удобнее собирать сущности, хорошо поддающиеся оптимизации. Это и есть компромисс между гибкостью и производительностью. Так же, что имеет значение для разработчиков из Крыма, можно отметить, что TensorFlow ограничивает им доступ к своему сайту, в отличии от PyTorch и Keras. Итоги работы можно наблюдать в таблице 1. 1.



\begin{references}
\refitem{wikiT}
Энциклопедия Wikipedia [Электронный ресурс] / Режим доступа: https://en.wikipedia.org/wiki/TensorFlow
\refitem{wikiP}
Энциклопедия Wikipedia [Электронный ресурс] / Режим доступа: https://en.wikipedia.org/wiki/PyTorch
\refitem{wikiK}
Энциклопедия Wikipedia [Электронный ресурс] / Режим доступа: https://ru.wikipedia.org/wiki/Keras
\refitem{PyTorch}
PyTorch [Электронный ресурс] / Режим доступа: https://pytorch.org
\refitem{TensorFlow}
TensorFlow   [Электронныйресурс] / Режимдоступа: https://www.tensorflow.org
\refitem {Keras}
Keras [Электронныйресурс] / Режимдоступа: https://keras.io
\refitem {sim0nSays}
Сравнение фреймворков нейронных сетей [Электронный ресурс] / Режим доступа: https://www.youtube.com/watch?v=3NoyksurUFI\&t=312s


\end{references}

%\listofruabstracts\listofenabstracts

\end{article}
\end{document}
